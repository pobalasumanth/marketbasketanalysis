{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Users With Similar Buying Habits and Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aisles_df = pd.read_csv('aisles.csv')\n",
    "dept_df = pd.read_csv('departments.csv')\n",
    "prodorder_prior_df = pd.read_csv('order_products__prior.csv')\n",
    "productorder_train_df = pd.read_csv('order_products__train.csv')\n",
    "order_df = pd.read_csv('orders.csv')\n",
    "product_df = pd.read_csv('products.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge into one dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the users that exist in both 'prior' table and train eval set of 'orders' table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders in prior merged with product names\n",
    "prodname_order_prior = pd.merge(prodorder_prior_df, \n",
    "                                    product_df, how='left', on='product_id')\n",
    "# Prior orders with user_id, product_id, product_name\n",
    "userorder_prior_prod = pd.merge(prodname_order_prior, \n",
    "                                    order_df, how='left', on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those that same user_id exists in both train and prior\n",
    "userorder_prior_prod_inner = pd.merge(userorder_prior_prod,\n",
    "                                          order_df[order_df['eval_set']=='train'][['user_id','eval_set']], \n",
    "                                          how='inner', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userorder_prior_prod_inner.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that will be extracted directly are:\n",
    "* Mean of order_dow (order placed day of week)\n",
    "* Mean of order_hour_of_day\n",
    "* Mean of days_since_prior_order\n",
    "* Total number of orders made\n",
    "* Total number of products bought\n",
    "\n",
    "Then we need another vectorized feature of product name: combine all the products name into one row per user, for word2Vector analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "habits_user = userorder_prior_prod_inner[['user_id','order_id',\n",
    "                                       'product_name','order_dow',\n",
    "                                       'order_hour_of_day','days_since_prior_order']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a dataframe: average value of each user\n",
    "user_average = habits_user.groupby('user_id')['order_dow',\n",
    "                                'order_hour_of_day',\n",
    "                                'days_since_prior_order'].agg(np.nanmean)\n",
    "# Total number of orders of each user\n",
    "user_order = habits_user.groupby('user_id').order_id.nunique()\n",
    "user_average['num_of_orders'] = user_order\n",
    "# Total number of products of each user\n",
    "prod_num = habits_user.groupby('user_id')['order_id'].agg('count')\n",
    "user_average['num_of_products'] = prod_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add product name to each user\n",
    "list_of_names = []\n",
    "for p_name in habits_user.groupby('user_id')['product_name']:\n",
    "        list_of_names.append(' '.join(p_name[1]))\n",
    "\n",
    "# add the names to dataframe\n",
    "user_average['product_name'] = list_of_names       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a glimpse of the dataset\n",
    "user_average.head()\n",
    "\n",
    "#userorder_prior_prod_inner.to_csv(\"../output/userorder_prior_prod_inner.csv\")\n",
    "#user_average.to_csv('../output/user_average.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Vectorized Text Feature: Use PySpark word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import word2Vector\n",
    "\n",
    "spark = SparkSession.builder.appName(\"User Habit\").getOrCreate()\n",
    "\n",
    "# load data\n",
    "# user_average = pd.read_csv(\"../output/user_average.csv\")\n",
    "prodname_df = pd.DataFrame(user_average['product_name'])\n",
    "# product_doc_df = spark.createDataFrame(prodname_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodname_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a fraction of data due to large computation\n",
    "fraction_sample = 0.2\n",
    "productname_sample_df  = prodname_df.sample(frac = fraction_sample, random_state=321)\n",
    "userid_sample  = productname_sample_df.index\n",
    "print(userid_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for row in productname_sample_df['product_name']:\n",
    "    tuple = (row.split(' '),)\n",
    "    df_list.append(tuple)\n",
    "\n",
    "# Check by the length of output\n",
    "print(len(df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data of word2Vector: Each row is a bag of words from a sentence or document.\n",
    "N = len(df_list)//100\n",
    "mod = len(df_list) % 100\n",
    "doc_df = spark.createDataFrame(df_list[0:100], [\"product_name\"])\n",
    "\n",
    "for i in range(1,N):\n",
    "    doc_df_sub = spark.createDataFrame(df_list[100*i:100*(i+1)], [\"product_name\"])\n",
    "    doc_df = doc_df.union(doc_df_sub)\n",
    "    \n",
    "doc_df_sub = spark.createDataFrame(df_list[100*N:len(df_list)], [\"product_name\"])\n",
    "doc_df = doc_df.union(doc_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=5, minCount=0, inputCol=\"product_name\", outputCol=\"res\")\n",
    "mdl = word2Vec.fit(doc_df)\n",
    "\n",
    "res = mdl.transform(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors: densevector\n",
    "features_vectored = [ ]\n",
    "for row in res.collect():\n",
    "    text, vector = row\n",
    "    features_vectored.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values from densevector into array\n",
    "features_vectored_array=[]\n",
    "for vectors in features_vectored:\n",
    "    features_vectored_array.append(vectors.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of vectorized feature columns\n",
    "column_names = []\n",
    "for i in range(1,6):\n",
    "    name = \"vectorized_feature_\" + str(i)\n",
    "    column_names.append(name)\n",
    "    \n",
    "# A dataframe: each vectorized feature as one column\n",
    "features_vectored_df = pd.DataFrame(np.array(features_vectored_array).reshape(len(df_list),5), \n",
    "                 columns = column_names)\n",
    "\n",
    "# Add \"user_id\" column\n",
    "features_vectored_df['user_id'] = userid_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a glimpse\n",
    "features_vectored_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All Features: Concatenate word2Vector feature with other features into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice user_average with sampled user_id\n",
    "sample_useravg = user_average[user_average.index.isin(userid_sample)]\n",
    "# set index as one column 'user_id'\n",
    "sample_useravg.reset_index(level=0, inplace=True)\n",
    "# merge two dfs on 'user_id'\n",
    "userfeatures_habits = pd.merge(sample_useravg, features_vectored_df, how='inner', on=\"user_id\")\n",
    "# drop \"product_name\"\n",
    "userfeatures_habits.drop('product_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a glimpse\n",
    "userfeatures_habits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#userfeatures_habits.to_csv('../output/userfeatures_habits.txt', sep='\\t', index=False)\n",
    "#userfeatures_habits.to_csv('../output/userfeatures_habits.csv', sep='\\t', index=False)\n",
    "#userfeatures_habits_only = userfeatures_habits.loc[:, 'order_dow':]\n",
    "#userfeatures_habits_only.to_csv('../output/userfeatures_habits_only.txt', sep='\\t', index=False)\n",
    "#userfeatures_habits_only.to_csv('../output/userfeatures_habits_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster Users: PySpark K-Means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA: Reduce features to 2-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "userfeatures_habits_only = pd.read_csv('../output/userfeatures_habits_only.csv')\n",
    "userfeatures_habits = pd.read_csv('../output/userfeatures_habits.csv', sep = '\\t')\n",
    "pca = PCA(n_components=2).fit(userfeatures_habits_only)\n",
    "pca_2d = pca.transform(userfeatures_habits_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dataframe = pd.DataFrame(pca_2d)\n",
    "#pca_dataframe.to_csv('../output/pca_feature_df.txt', sep='\\t', index=False)\n",
    "#pca_dataframe.to_csv('../output/pca_feature_df.csv', index=False)\n",
    "pca_dataframe['user_id'] = userfeatures_habits['user_id']\n",
    "#pca_dataframe.to_csv('../output/pca_dataframe.txt', sep='\\t', index=False)\n",
    "#pca_dataframe.to_csv('../output/pca_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the optimal K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal number of clusters by calculating the within set sum of squared error (WSSSE). As the number of cluster increases, WSSSE will decrease. The best choice is at the elbow of WSSSE graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "datapca = sc.textFile(\"../output/pca_feature_df.txt\")\n",
    "parseddatapca = datapca.map(lambda line: array([float(x) for x in line.split('\\t')]))\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSSSE_listpca = []\n",
    "\n",
    "K_range = range(5,185,5)\n",
    "for K in K_range:\n",
    "    \n",
    "    # Build the mdl (cluster the data)\n",
    "    clusters = KMeans.train(parseddatapca, K, maxIterations=10, initializationMode=\"random\")\n",
    "    \n",
    "    WSSSE_pca = parseddatapca.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"====== k:\"+str(K)+\" -- Within Set Sum of Squared Error = \" + str(WSSSE_pca) + \"=======\")\n",
    "    WSSSE_listpca.append(WSSSE_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSSSE_datapca = {'K':K_range, \"WSSSE\": WSSSE_listpca}\n",
    "WSSSE_pca_dataframe = pd.DataFrame(WSSSE_datapca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "WSSSE_pca_dataframe.plot(x='K', y='WSSSE')\n",
    "plt.axvline(40, \n",
    "            color='darkorange', linestyle='dashed', linewidth=2)\n",
    "plt.xlabel('Clusters')\n",
    "plt.title('Within Set Sum of Squared Error of K-Means')\n",
    "plt.show()\n",
    "#fig.set_dpi(200)\n",
    "#fig.savefig(\"../figs/WSSSE_pca.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal k is usually one where there is an “elbow” in the WSSSE graph. So choose k = 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run K-Means mdl with optimal K=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_optimal = 40\n",
    "clusters = KMeans.train(parseddatapca, k_optimal, maxIterations=10, initializationMode=\"random\")\n",
    "#clusters.save(sc, \"../output/KMeansmdl_pca\")\n",
    "#clusters = KMeansmdl.load(sc, \"../output/KMeansmdl_pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_clusters = clusters.predict(parseddatapca)\n",
    "# Into a list\n",
    "cluster_res = [ ]\n",
    "for row in predict_clusters.collect():\n",
    "    cluster_res.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the centers for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCenter(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return center\n",
    "\n",
    "RDDCenter = parseddatapca.map(lambda point: GetCenter(point))\n",
    "\n",
    "ress_center = [ ]\n",
    "for row in RDDCenter.collect():\n",
    "    ress_center.append(row)\n",
    "    \n",
    "ress_center = pd.DataFrame(ress_center,columns=['x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans ress Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_kmeans = ress_center\n",
    "summary_kmeans['clusters'] = cluster_res\n",
    "summary_kmeans['user_id'] = userfeatures_habits['user_id']\n",
    "\n",
    "summary_kmeans = pd.merge(pca_dataframe, summary_kmeans ,how='inner', on='user_id')\n",
    "#summary_kmeans.to_csv(\"../output/summary_kmeans.csv\", header=True)\n",
    "\n",
    "# x & y are coordinates of cluster centers\n",
    "# 0 & 1 are coordinates of each user\n",
    "summary_kmeans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Kmeans ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Unique category labels for clusters\n",
    "labels_color = summary_kmeans['clusters'].unique()\n",
    "\n",
    "# List of RGB triplets\n",
    "rgb_values = sns.color_palette(\"Set2\", 40)\n",
    "\n",
    "# Map label to RGB\n",
    "map_color = dict(zip(labels_color, rgb_values))\n",
    "\n",
    "# Finally use the mapped values\n",
    "plt.scatter(summary_kmeans['x'], summary_kmeans['y'], c=summary_kmeans['clusters'].map(map_color))\n",
    "plt.title(\"Centers for K-Means Clusters\")\n",
    "plt.show()\n",
    "\n",
    "#fig.set_dpi(200)\n",
    "#fig.savefig('../figs/cluster_centers.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Unique category labels for clusters\n",
    "labels_color = summary_kmeans['clusters'].unique()\n",
    "\n",
    "# List of RGB triplets\n",
    "rgb_values = sns.color_palette(\"Set2\", 40)\n",
    "\n",
    "# Map label to RGB\n",
    "map_color = dict(zip(labels_color, rgb_values))\n",
    "\n",
    "\n",
    "# Finally use the mapped values\n",
    "plt.scatter(summary_kmeans[0], summary_kmeans[1], c=summary_kmeans['clusters'].map(map_color), s = 0.5)\n",
    "plt.title(\"K-Means Clusters\")\n",
    "plt.show()\n",
    "\n",
    "#fig.set_dpi(300)\n",
    "#fig.savefig('../figs/clusters.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Popular Products in Each User Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "summary_kmeans = pd.read_csv(\"../output/summary_kmeans.csv\")\n",
    "\n",
    "# merge to get clusters corresponds to product_name\n",
    "cluster_order_info = pd.merge(summary_kmeans, userorder_prior_prod_inner, how='left', on='user_id')\n",
    "prod_cluster = cluster_order_info[['user_id','clusters','product_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of products in each cluster\n",
    "count_cluster = prod_cluster.groupby(['clusters','product_name']).agg('count')\n",
    "# reset indexes (twice)\n",
    "# count_cluster.reset_index(level=0, inplace=True)\n",
    "# count_cluster.reset_index(level=0, inplace=True)\n",
    "# count_cluster.sort_values(['clusters','user_id'], ascending=False).groupby('clusters').head(5)\n",
    "\n",
    "# Top 10 products in each cluster\n",
    "top_prods = count_cluster['user_id'].groupby(level=0, group_keys=False).nlargest(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#top_prods.columns.values[2]='count'\n",
    "top_prods[top_prods['clusters'] == 0][['product_name','user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widetop_products =top_prods.pivot(index='clusters', columns='product_name', values='user_id').fillna(0)\n",
    "widetop_products_percent = widetop_products.div(widetop_products.sum(axis=0), axis=1)\n",
    "longtop_products = widetop_products_percent.unstack().reset_index()\n",
    "longtop_products.columns.values[2]='count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,figsize=(20,40),sharey=True)\n",
    "#plt.figure(figsize=(25,10))\n",
    "#ax.spines['top'].set_visible(False)\n",
    "#ax.spines['right'].set_visible(False)\n",
    "#ax.spines['bottom'].set_visible(False)\n",
    "#ax.spines['left'].set_visible(False)\n",
    "#ax.patch.set_visible(False)\n",
    "#ax.grid(False)\n",
    "\n",
    "grp1 = []\n",
    "for i in range(10):\n",
    "    grp1.append(i)\n",
    "ax1.plot(widetop_products_percent.loc[grp1].transpose())\n",
    "ax1.legend(widetop_products_percent.transpose().columns[0:10],title=\"Cluster ID\",loc='upper right',prop={'size': 12})\n",
    "ax1.set_title('Percent of Products in Each Cluster',size=20)\n",
    "#ax1.set_xticklabels(rotation=90, size=12)\n",
    "\n",
    "grp2 = []\n",
    "for i in range(10,20):\n",
    "    grp2.append(i)\n",
    "ax2.plot(widetop_products_percent.loc[grp2].transpose())\n",
    "ax2.legend(widetop_products_percent.transpose().columns[10:20], title=\"Cluster ID\",loc='upper right',prop={'size': 12})\n",
    "#ax2.set_xticklabels(rotation=90, size=12)\n",
    "\n",
    "grp3 = []\n",
    "for i in range(20,30):\n",
    "    grp3.append(i)\n",
    "ax3.plot(widetop_products_percent.loc[grp3].transpose())\n",
    "ax3.legend(widetop_products_percent.transpose().columns[20:30],title=\"Cluster ID\",loc='upper right',prop={'size': 12})\n",
    "#ax3.set_xticklabels(rotation=90, size=12)\n",
    "\n",
    "grp4 = []\n",
    "for i in range(30,40):\n",
    "    grp4.append(i)\n",
    "ax4.plot(widetop_products_percent.loc[grp4].transpose())\n",
    "ax4.legend(widetop_products_percent.transpose().columns[30:40],title=\"Cluster ID\",loc='upper right',prop={'size': 12})\n",
    "#ax4.set_xticklabels(rotation=90, size=12)\n",
    "#plt.xticks(rotation=90, size=12)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=90, size=12)\n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0.7)\n",
    "#fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "fig.set_dpi(300)\n",
    "fig.savefig('../figs/prod_cluster_frequency.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
